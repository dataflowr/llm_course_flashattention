{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0c5a0c2-9d32-4303-94d2-ffacd9812ef7",
   "metadata": {},
   "source": [
    "```\n",
    "      ___           ___       ___           ___           ___                                                               \n",
    "     /\\  \\         /\\__\\     /\\  \\         /\\  \\         /\\__\\                                                              \n",
    "    /::\\  \\       /:/  /    /::\\  \\       /::\\  \\       /:/  /                                                              \n",
    "   /:/\\:\\  \\     /:/  /    /:/\\:\\  \\     /:/\\ \\  \\     /:/__/                                                               \n",
    "  /::\\~\\:\\  \\   /:/  /    /::\\~\\:\\  \\   _\\:\\~\\ \\  \\   /::\\  \\ ___                                                           \n",
    " /:/\\:\\ \\:\\__\\ /:/__/    /:/\\:\\ \\:\\__\\ /\\ \\:\\ \\ \\__\\ /:/\\:\\  /\\__\\                                                          \n",
    " \\/__\\:\\ \\/__/ \\:\\  \\    \\/__\\:\\/:/  / \\:\\ \\:\\ \\/__/ \\/__\\:\\/:/  /                                                          \n",
    "      \\:\\__\\    \\:\\  \\        \\::/  /   \\:\\ \\:\\__\\        \\::/  /                                                           \n",
    "       \\/__/     \\:\\  \\       /:/  /     \\:\\/:/  /        /:/  /                                                            \n",
    "                  \\:\\__\\     /:/  /       \\::/  /        /:/  /                                                             \n",
    "                   \\/__/     \\/__/         \\/__/         \\/__/                                                              \n",
    "      ___           ___           ___           ___           ___           ___                       ___           ___     \n",
    "     /\\  \\         /\\  \\         /\\  \\         /\\  \\         /\\__\\         /\\  \\          ___        /\\  \\         /\\__\\    \n",
    "    /::\\  \\        \\:\\  \\        \\:\\  \\       /::\\  \\       /::|  |        \\:\\  \\        /\\  \\      /::\\  \\       /::|  |   \n",
    "   /:/\\:\\  \\        \\:\\  \\        \\:\\  \\     /:/\\:\\  \\     /:|:|  |         \\:\\  \\       \\:\\  \\    /:/\\:\\  \\     /:|:|  |   \n",
    "  /::\\~\\:\\  \\       /::\\  \\       /::\\  \\   /::\\~\\:\\  \\   /:/|:|  |__       /::\\  \\      /::\\__\\  /:/  \\:\\  \\   /:/|:|  |__ \n",
    " /:/\\:\\ \\:\\__\\     /:/\\:\\__\\     /:/\\:\\__\\ /:/\\:\\ \\:\\__\\ /:/ |:| /\\__\\     /:/\\:\\__\\  __/:/\\/__/ /:/__/ \\:\\__\\ /:/ |:| /\\__\\\n",
    " \\/__\\:\\/:/  /    /:/  \\/__/    /:/  \\/__/ \\:\\~\\:\\ \\/__/ \\/__|:|/:/  /    /:/  \\/__/ /\\/:/  /    \\:\\  \\ /:/  / \\/__|:|/:/  /\n",
    "      \\::/  /    /:/  /        /:/  /       \\:\\ \\:\\__\\       |:/:/  /    /:/  /      \\::/__/      \\:\\  /:/  /      |:/:/  / \n",
    "      /:/  /     \\/__/         \\/__/         \\:\\ \\/__/       |::/  /     \\/__/        \\:\\__\\       \\:\\/:/  /       |::/  /  \n",
    "     /:/  /                                   \\:\\__\\         /:/  /                    \\/__/        \\::/  /        /:/  /   \n",
    "     \\/__/                                     \\/__/         \\/__/                                   \\/__/         \\/__/    \n",
    "```\n",
    "\n",
    "```                                                                                                              \n",
    " ██▓ ███▄    █    ▄▄▄█████▓ ██▀███   ██▓▄▄▄█████▓ ▒█████   ███▄    █ \n",
    "▓██▒ ██ ▀█   █    ▓  ██▒ ▓▒▓██ ▒ ██▒▓██▒▓  ██▒ ▓▒▒██▒  ██▒ ██ ▀█   █ \n",
    "▒██▒▓██  ▀█ ██▒   ▒ ▓██░ ▒░▓██ ░▄█ ▒▒██▒▒ ▓██░ ▒░▒██░  ██▒▓██  ▀█ ██▒\n",
    "░██░▓██▒  ▐▌██▒   ░ ▓██▓ ░ ▒██▀▀█▄  ░██░░ ▓██▓ ░ ▒██   ██░▓██▒  ▐▌██▒\n",
    "░██░▒██░   ▓██░     ▒██▒ ░ ░██▓ ▒██▒░██░  ▒██▒ ░ ░ ████▓▒░▒██░   ▓██░\n",
    "░▓  ░ ▒░   ▒ ▒      ▒ ░░   ░ ▒▓ ░▒▓░░▓    ▒ ░░   ░ ▒░▒░▒░ ░ ▒░   ▒ ▒ \n",
    " ▒ ░░ ░░   ░ ▒░       ░      ░▒ ░ ▒░ ▒ ░    ░      ░ ▒ ▒░ ░ ░░   ░ ▒░\n",
    " ▒ ░   ░   ░ ░      ░        ░░   ░  ▒ ░  ░      ░ ░ ░ ▒     ░   ░ ░ \n",
    " ░           ░                ░      ░               ░ ░           ░ \n",
    "                                                                                                                        \n",
    "```\n",
    "- by [Marc Lelarge](https://www.di.ens.fr/~lelarge/) - [@marc_lelarge](https://x.com/marc_lelarge)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41532c20-4403-498a-8918-de048cca1f1d",
   "metadata": {},
   "source": [
    "# Standard Softmax\n",
    "\n",
    "The **softmax operator** transforms a vector of real numbers into a probability distribution. For $x_1,\\dots, x_d \\in \\mathbb{R}$, softmax is defined as:\n",
    "$$\n",
    "\\mathrm{softmax}(x_1, \\ldots, x_d)\n",
    "=\n",
    "\\left(\n",
    "\\frac{e^{x_i}}{\\sum_{j=1}^{d} e^{x_j}}\n",
    "\\right)_{i=1}^{d}\n",
    "$$\n",
    "The output satisfies $\\sum_{i=1}^d \\mathrm{softmax}_i(x_1, \\ldots, x_d) = 1$.\n",
    "\n",
    "## Numerical Stability: Safe Softmax\n",
    "\n",
    "Since $x_i$ may be large, computing $e^{x_i}$ directly can cause numerical overflow. To address this, we use the **safe softmax** trick, which exploits the translation invariance of softmax:\n",
    "$$\n",
    "\\frac{e^{x_i}}{\\sum_{j=1}^{d} e^{x_j}}\n",
    "=\n",
    "\\frac{e^{x_i - m}}{\\sum_{j=1}^{d} e^{x_j - m}}\n",
    "$$\n",
    "where\n",
    "$$\n",
    "m = \\max_{j=1,\\ldots,d} x_j\n",
    "$$\n",
    "This ensures $x_i - m \\leq 0$ for all $i$, making the exponential computation numerically stable since $e^{x_i - m} \\in (0, 1]$.\n",
    "\n",
    "## Matrix Extension\n",
    "\n",
    "For matrices $x \\in \\mathbb{R}^{d_1 \\times d_2}$, we extend softmax by applying it **column-wise**. Each $x_i \\in \\mathbb{R}^{d_1}$ denotes the $i$-th column of $x$ (for $i=1,\\ldots,d_2$), and we compute:\n",
    "$$\n",
    "\\mathrm{softmax}(x)_{ij} = \\frac{e^{x_{ij} - m_i}}{\\sum_{k=1}^{d_2} e^{x_{ik} - m_i}}\n",
    "$$\n",
    "where $m_i = \\max_{k=1,\\ldots,d_2} x_{ik}$ is computed along the $i$-th row. For each row index $i$, the outputs form a probability distribution over the columns:\n",
    "$$\n",
    "\\sum_{j=1}^{d_2} \\mathrm{softmax}(x)_{ij} = 1\n",
    "$$\n",
    "To simplify notation, we will use all operations component wise so that we can still write \n",
    "$$\n",
    "\\mathrm{softmax}(x_1, \\ldots, x_{d_2})\n",
    "=\n",
    "\\left(\n",
    "\\frac{e^{x_i- m}}{\\sum_{j=1}^{d_2} e^{x_j- m}}\n",
    "\\right)_{i=1}^{d_2},\n",
    "$$\n",
    "where $e^{x_i}\\in \\mathbb{R}^{d_1}$ is applied component-wise like the maximum $m =\\max_{j=1,\\ldots,d_2} x_j \\in \\mathbb{R}^{d_1}$.\n",
    "\n",
    "The naive implementation of the safe softmax requires 3 passes on the data $x$: the first pass compute the max $m$, the second pass compute the denominator $\\ell = \\sum_{j=1}^{d_2} e^{x_j- m}$ and the last pass compute the softmax. \n",
    "\n",
    "## Algorithm: (safe) softmax\n",
    "\n",
    "**Input:** $x_1, \\ldots, x_{d_2} \\in \\mathbb{R^{d_1}}$  \n",
    "**Output:** $s_1, \\ldots, s_{d_2}$ (softmax values)\n",
    "\n",
    "**Step 1:** Compute the maximum\n",
    "$$\n",
    "\\begin{align}\n",
    "m_0 &= -\\infty \\\\\n",
    "m_i &\\leftarrow \\max(m_{i-1}, x_i) \\quad \\text{for } i = 1, \\ldots, d_2 \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "**Step 2:** Compute the normalizing constant\n",
    "$$\n",
    "\\begin{align}\n",
    "\\ell_0 &= 0 \\\\\n",
    "\\ell_i &\\leftarrow \\ell_{i-1} + e^{x_i - m_{d_2}} \\quad \\text{for } i = 1, \\ldots, d_2\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "**Step 3:** Compute softmax values\n",
    "$$\n",
    "s_i \\leftarrow \\frac{e^{x_i - m_{d_2}}}{\\ell_{d_2}} \\quad \\text{for } i = 1, \\ldots, d_2\n",
    "$$\n",
    "Below is the corresponding code in PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d42e39-80c9-4539-8aa1-d8892b456e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7026e6ee-8d35-4d96-aaef-cde16aa4e47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x, dim=-1):\n",
    "    rescaled_input = x - torch.max(x, dim=dim, keepdim=True)[0]\n",
    "    exponentiated_rescaled_input = torch.exp(rescaled_input)\n",
    "    return exponentiated_rescaled_input / torch.sum(\n",
    "        exponentiated_rescaled_input, dim=dim, keepdim=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb896ddb-17d8-49a1-8c4f-1fdfec1948ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(8,16)\n",
    "y = softmax(x)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ae9524-1e77-4f94-8aa1-2141dca1941f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ATOL = RTOL = 1e-5\n",
    "torch.testing.assert_close(y.sum(-1), torch.ones(x.shape[0]), atol=ATOL, rtol=RTOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38ec22f-4d21-45c8-a7cb-8035456934c5",
   "metadata": {},
   "source": [
    "# Online Softmax\n",
    "\n",
    "Naive implementation requires 3 global memory accesses, we would like to decrease it. In order to compute $\\ell_i = \\sum_{j\\leq i} e^{x_j-m_{d_2}}$, we need to have made one pass in order to compute $m_{d_2}$.\n",
    "Instead, define $\\ell'_i = \\sum_{j\\leq i} e^{x_j-m_i}$ so that we get:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\ell'_i &= \\left( \\sum_{j\\leq i-1} e^{x_j-m_{i-1}} \\right) e^{m_{i-1}-m_i} + e^{x_i-m_i}\\\\\n",
    "&= \\ell'_{i-1}e^{m_{i-1}-m_i} + e^{x_i-m_i},\n",
    "\\end{align}\n",
    "$$\n",
    "and of course $\\ell'_{d_2} = \\ell_{d_2}$ the desired normalizing constant. Since computing $\\ell'_i$ only requires only the quantities $m_{i-1}, m_i$ and $x_i$, it can be computed during the first pass.\n",
    "\n",
    "## Algorithm: online softmax\n",
    "\n",
    "**Input:** $x_1, \\ldots, x_{d_2} \\in \\mathbb{R^{d_1}}$  \n",
    "**Output:** $s_1, \\ldots, s_{d_2}$ (softmax values)\n",
    "\n",
    "**Step 1:** Compute the maximum and the normalizing constant\n",
    "$$\n",
    "\\begin{align}\n",
    "m_0 &= -\\infty \\\\\n",
    "m_i &\\leftarrow \\max(m_{i-1}, x_i) \\quad \\text{for } i = 1, \\ldots, d_2 \\\\\n",
    "\\ell'_0 &= 0 \\\\\n",
    "\\ell'_i &\\leftarrow \\ell'_{i-1}e^{m_{i-1}-m_i} + e^{x_i - m_i} \\quad \\text{for } i = 1, \\ldots, d_2\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "**Step 3:** Compute softmax values\n",
    "$$\n",
    "s_i \\leftarrow \\frac{e^{x_i - m_{d_2}}}{\\ell'_{d_2}} \\quad \\text{for } i = 1, \\ldots, d_2\n",
    "$$\n",
    "Below is the corresponding code in PyTorch (parameter `B` allows to make computations (taking the maximum and summing) in \"blocks\"):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9609f30-0773-45e8-bc51-e9088ee1e3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def online_softmax(x, B=1):\n",
    "    *bs, d = x.shape\n",
    "    device = x.device\n",
    "    Td = math.ceil(d / B)\n",
    "    m_prev = torch.full((*bs, 1), float(\"-inf\"), device=device)  # current max\n",
    "    l_prev = torch.zeros((*bs, 1), device=device)  # current sum of exps\n",
    "\n",
    "    for i in range(Td):\n",
    "        start = i * B\n",
    "        end = min((i + 1) * B, d)\n",
    "        x_block = x[..., start:end]\n",
    "\n",
    "        block_max = x_block.max(dim=-1, keepdim=True).values\n",
    "        m_curr = torch.maximum(m_prev, block_max)\n",
    "\n",
    "        l_prev = l_prev * torch.exp(m_prev - m_curr) + torch.exp(x_block - m_curr).sum(\n",
    "            dim=-1, keepdim=True\n",
    "        )\n",
    "        m_prev = m_curr\n",
    "\n",
    "    softmax_output = torch.empty_like(x)\n",
    "    for i in range(Td):\n",
    "        start = i * B\n",
    "        end = min((i + 1) * B, d)\n",
    "        softmax_output[..., start:end] = torch.exp(x[..., start:end] - m_prev)\n",
    "\n",
    "    return softmax_output / l_prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248df682-95c0-426e-b1de-14fc854b4e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.testing.assert_close(y, online_softmax(x), atol=ATOL, rtol=RTOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16321bf7-2603-4283-b584-fe36e13f9a95",
   "metadata": {},
   "source": [
    "# Triton implementation of online softmax\n",
    "\n",
    "Before using [Triton](https://triton-lang.org/main/index.html), we give a PyTorch implementaion to highlight the main concepts to code in Triton. For more details, we recommend to have a look at the notebook: [Programming on GPUs](https://github.com/dataflowr/notebooks/blob/master/ModuleGPU/GPU_programming_basics.ipynb)\n",
    "\n",
    "## Triton Programming Model\n",
    "\n",
    "Triton uses an **SPMD (Single Program, Multiple Data)** approach where the same kernel code runs in parallel across multiple \"program instances,\" each processing a different block of data.\n",
    "\n",
    "**Key concepts illustrated in the (PyTorch) code below:**\n",
    "\n",
    "1. **Program ID (`fake_pid`)**: Each kernel instance has a unique program ID that determines which data block it processes. In the code, `fake_pid = (i, j)` identifies the batch index `i` and tile index `j`.\n",
    "\n",
    "2. **Block/Tile-based processing**: The input matrix is divided into tiles (e.g., `block_1 × block_2`). Each program instance loads and processes one tile:\n",
    "```python\n",
    "   x_block = x_ptr[fake_pid[0], fake_pid[1] * block_1 : (fake_pid[1] + 1) * block_1, start:end]\n",
    "```\n",
    "\n",
    "3. **Parallel execution**: Multiple kernel instances run concurrently, each with a different `pid`. The shuffling in `online_softmax_fake_triton` simulates this parallelism:\n",
    "```python\n",
    "   all_blocks = [(i, j) for i in range(bs) for j in range(Num_tiles)]\n",
    "   random.shuffle(all_blocks)  # Simulates arbitrary execution order\n",
    "```\n",
    "\n",
    "4. **Memory hierarchy**: Blocks are sized to fit in GPU SRAM (Triton's fast shared memory), though this Python simulation doesn't explicitly model the memory transfer.\n",
    "\n",
    "The programmer writes a single kernel function that operates on one block, and Triton automatically launches many parallel instances across all blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fac2c0-d1ba-43d5-bbcd-e185b8c0a69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def online_softmax_kernel(x_ptr, block_1, block_2, d_2, fake_pid):\n",
    "    device = x_ptr.device\n",
    "    Num_blocks = math.ceil(d_2 / block_2)\n",
    "\n",
    "    m_prev = torch.full((block_1, 1), float(\"-inf\"), device=device)  # current max\n",
    "    l_prev = torch.zeros((block_1, 1), device=device)  # current sum of exps\n",
    "\n",
    "    for i in range(Num_blocks):\n",
    "        start = i * block_2\n",
    "        end = min((i + 1) * block_2, d_2)\n",
    "        x_block = x_ptr[\n",
    "            fake_pid[0], fake_pid[1] * block_1 : (fake_pid[1] + 1) * block_1, start:end\n",
    "        ]\n",
    "\n",
    "        block_max = x_block.max(dim=-1, keepdim=True).values\n",
    "        m_curr = torch.maximum(m_prev, block_max)\n",
    "\n",
    "        l_prev = l_prev * torch.exp(m_prev - m_curr) + torch.exp(x_block - m_curr).sum(\n",
    "            dim=-1, keepdim=True\n",
    "        )\n",
    "        m_prev = m_curr\n",
    "\n",
    "    softmax_output = torch.empty_like(\n",
    "        x_ptr[fake_pid[0], fake_pid[1] * block_1 : (fake_pid[1] + 1) * block_1, :]\n",
    "    )\n",
    "    for i in range(Num_blocks):\n",
    "        start = i * block_2\n",
    "        end = min((i + 1) * block_2, d_2)\n",
    "        x_block = x_ptr[\n",
    "            fake_pid[0], fake_pid[1] * block_1 : (fake_pid[1] + 1) * block_1, start:end\n",
    "        ]\n",
    "        softmax_output[..., start:end] = torch.exp(x_block - m_prev)\n",
    "\n",
    "    return softmax_output / l_prev\n",
    "\n",
    "\n",
    "def online_softmax_fake_triton(x, B=16):\n",
    "    assert x.shape[1] % B == 0, \"d1 must be a multiple of B for fake triton kernel\"\n",
    "    bs, d1, d2 = x.shape\n",
    "    Num_tiles = math.ceil(d1 / B)\n",
    "    softmax_output = torch.empty_like(x)\n",
    "\n",
    "    # Create list of all (batch, tile) pairs and shuffle to show parallelism\n",
    "    all_blocks = [(i, j) for i in range(bs) for j in range(Num_tiles)]\n",
    "    random.shuffle(all_blocks) # Simulates arbitrary execution order\n",
    "\n",
    "    for i, j in all_blocks:\n",
    "        fake_pid = (i, j)\n",
    "        softmax_output[i, j * B : min((j + 1) * B, d1), :] = online_softmax_kernel(\n",
    "            x, B, B, d2, fake_pid\n",
    "        )\n",
    "    return softmax_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8b561c-09c2-4677-a403-e4ddf2163961",
   "metadata": {},
   "outputs": [],
   "source": [
    "B = 8\n",
    "x = torch.randn(2,8,16)\n",
    "y = softmax(x)\n",
    "torch.testing.assert_close(y, online_softmax_fake_triton(x, B=B), atol=ATOL, rtol=RTOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e9bf61-cbc2-4d8d-84ab-5bd252963adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import triton\n",
    "import triton.language as tl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e1fe7e-6499-4a30-9b02-30bcdd9065f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@triton.jit\n",
    "def online_softmax_triton_kernel(\n",
    "    x_ptr,\n",
    "    softmax_ptr,\n",
    "    stride_xbatch,\n",
    "    stride_xrow,\n",
    "    stride_xcol,\n",
    "    stride_sbatch,\n",
    "    stride_srow,\n",
    "    stride_scol,\n",
    "    d1: tl.constexpr,\n",
    "    d2: tl.constexpr,\n",
    "    BLOCK_1: tl.constexpr,\n",
    "    BLOCK_2: tl.constexpr,\n",
    "):\n",
    "\n",
    "    tl.static_assert(d2 % BLOCK_2 == 0, \"d2 must be divisible by BLOCK_2\")\n",
    "    tl.static_assert(d1 % BLOCK_1 == 0, \"d1 must be divisible by BLOCK_1\")\n",
    "\n",
    "    # Each program handles one block of rows (BLOCK_1 rows)\n",
    "    pid_batch = tl.program_id(0)\n",
    "    pid_row = tl.program_id(1)\n",
    "\n",
    "    x_block = tl.make_block_ptr(\n",
    "        x_ptr + pid_batch * stride_xbatch,\n",
    "        shape=(d1, d2),\n",
    "        strides=(stride_xrow, stride_xcol),\n",
    "        offsets=(pid_row * BLOCK_1, 0),\n",
    "        block_shape=(BLOCK_1, BLOCK_2),\n",
    "        order=(1, 0),\n",
    "    )\n",
    "\n",
    "    # Number of blocks in the column dimension\n",
    "    Num_blocks = tl.cdiv(d2, BLOCK_2)\n",
    "\n",
    "    # Initialize m_prev and l_prev for this block of rows\n",
    "    m_prev = tl.full((BLOCK_1,), float(\"-inf\"), dtype=tl.float32)\n",
    "    l_prev = tl.zeros((BLOCK_1,), dtype=tl.float32)\n",
    "\n",
    "    # First pass: compute global max and sum\n",
    "    for _ in range(Num_blocks):\n",
    "        x = tl.load(x_block, boundary_check=(0, 1), padding_option=\"zero\")\n",
    "\n",
    "        # Compute block max\n",
    "        block_max = tl.max(x, axis=1)\n",
    "        m_curr = tl.maximum(m_prev, block_max)\n",
    "\n",
    "        # Update running sum with rescaling\n",
    "        exp_x_block = tl.exp(x - m_curr[:, None])\n",
    "        l_prev = l_prev * tl.exp(m_prev - m_curr) + tl.sum(exp_x_block, axis=1)\n",
    "        m_prev = m_curr\n",
    "\n",
    "        x_block = x_block.advance((0, BLOCK_2))\n",
    "\n",
    "    # Second pass: compute and store softmax output\n",
    "    softmax_block = tl.make_block_ptr(\n",
    "        softmax_ptr + pid_batch * stride_sbatch,\n",
    "        shape=(d1, d2),\n",
    "        strides=(stride_srow, stride_scol),\n",
    "        offsets=(pid_row * BLOCK_1, 0),\n",
    "        block_shape=(BLOCK_1, BLOCK_2),\n",
    "        order=(1, 0),\n",
    "    )\n",
    "    x_block = tl.make_block_ptr(\n",
    "        x_ptr + pid_batch * stride_xbatch,\n",
    "        shape=(d1, d2),\n",
    "        strides=(stride_xrow, stride_xcol),\n",
    "        offsets=(pid_row * BLOCK_1, 0),\n",
    "        block_shape=(BLOCK_1, BLOCK_2),\n",
    "        order=(1, 0),\n",
    "    )\n",
    "    # tl.device_print(\"m_prev:\", m_prev)\n",
    "    # tl.device_print(\"l_prev:\", l_prev)\n",
    "\n",
    "    for _ in range(Num_blocks):\n",
    "        x = tl.load(x_block, boundary_check=(0, 1), padding_option=\"zero\")\n",
    "        # Compute softmax for this block\n",
    "        tl.store(\n",
    "            softmax_block,\n",
    "            tl.exp(x - m_prev[:, None]) / l_prev[:, None],\n",
    "            boundary_check=(0, 1),\n",
    "        )\n",
    "\n",
    "        x_block = x_block.advance((0, BLOCK_2))\n",
    "        softmax_block = softmax_block.advance((0, BLOCK_2))\n",
    "\n",
    "\n",
    "def online_softmax_triton(x, BLOCK_1=16, BLOCK_2=16):\n",
    "    \"\"\"\n",
    "    Compute softmax using Triton kernel with online algorithm.\n",
    "\n",
    "    Args:\n",
    "        x: Input tensor of shape (batch_size, d1, d2)\n",
    "        BLOCK_1: Block size for dimension d1 (rows)\n",
    "        BLOCK_2: Block size for dimension d2 (columns, softmax dimension)\n",
    "    \"\"\"\n",
    "    batch_size, d1, d2 = x.shape\n",
    "    softmax_output = torch.empty_like(x)\n",
    "\n",
    "    # Calculate grid dimensions\n",
    "    grid = (batch_size, triton.cdiv(d1, BLOCK_1))\n",
    "\n",
    "    # Launch kernel\n",
    "    online_softmax_triton_kernel[grid](\n",
    "        x,\n",
    "        softmax_output,\n",
    "        x.stride(0),\n",
    "        x.stride(1),\n",
    "        x.stride(2),\n",
    "        softmax_output.stride(0),\n",
    "        softmax_output.stride(1),\n",
    "        softmax_output.stride(2),\n",
    "        d1,\n",
    "        d2,\n",
    "        BLOCK_1=BLOCK_1,\n",
    "        BLOCK_2=BLOCK_2,\n",
    "    )\n",
    "\n",
    "    return softmax_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6f7e1d-27cd-44e0-93f3-b287c66dea36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dldiy",
   "language": "python",
   "name": "dldiy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
