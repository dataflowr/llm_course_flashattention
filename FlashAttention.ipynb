{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0c5a0c2-9d32-4303-94d2-ffacd9812ef7",
   "metadata": {},
   "source": [
    "```\n",
    " ▄▄▄▄▄▄▄▄▄▄▄  ▄            ▄▄▄▄▄▄▄▄▄▄▄  ▄▄▄▄▄▄▄▄▄▄▄  ▄         ▄                                                     \n",
    "▐░░░░░░░░░░░▌▐░▌          ▐░░░░░░░░░░░▌▐░░░░░░░░░░░▌▐░▌       ▐░▌                                                    \n",
    "▐░█▀▀▀▀▀▀▀▀▀ ▐░▌          ▐░█▀▀▀▀▀▀▀█░▌▐░█▀▀▀▀▀▀▀▀▀ ▐░▌       ▐░▌                                                    \n",
    "▐░▌          ▐░▌          ▐░▌       ▐░▌▐░▌          ▐░▌       ▐░▌                                                    \n",
    "▐░█▄▄▄▄▄▄▄▄▄ ▐░▌          ▐░█▄▄▄▄▄▄▄█░▌▐░█▄▄▄▄▄▄▄▄▄ ▐░█▄▄▄▄▄▄▄█░▌                                                    \n",
    "▐░░░░░░░░░░░▌▐░▌          ▐░░░░░░░░░░░▌▐░░░░░░░░░░░▌▐░░░░░░░░░░░▌                                                    \n",
    "▐░█▀▀▀▀▀▀▀▀▀ ▐░▌          ▐░█▀▀▀▀▀▀▀█░▌ ▀▀▀▀▀▀▀▀▀█░▌▐░█▀▀▀▀▀▀▀█░▌                                                    \n",
    "▐░▌          ▐░▌          ▐░▌       ▐░▌          ▐░▌▐░▌       ▐░▌                                                    \n",
    "▐░▌          ▐░█▄▄▄▄▄▄▄▄▄ ▐░▌       ▐░▌ ▄▄▄▄▄▄▄▄▄█░▌▐░▌       ▐░▌                                                    \n",
    "▐░▌          ▐░░░░░░░░░░░▌▐░▌       ▐░▌▐░░░░░░░░░░░▌▐░▌       ▐░▌                                                    \n",
    " ▀            ▀▀▀▀▀▀▀▀▀▀▀  ▀         ▀  ▀▀▀▀▀▀▀▀▀▀▀  ▀         ▀                                                     \n",
    "                                                                                                                     \n",
    " ▄▄▄▄▄▄▄▄▄▄▄  ▄▄▄▄▄▄▄▄▄▄▄  ▄▄▄▄▄▄▄▄▄▄▄  ▄▄▄▄▄▄▄▄▄▄▄  ▄▄        ▄  ▄▄▄▄▄▄▄▄▄▄▄  ▄▄▄▄▄▄▄▄▄▄▄  ▄▄▄▄▄▄▄▄▄▄▄  ▄▄        ▄ \n",
    "▐░░░░░░░░░░░▌▐░░░░░░░░░░░▌▐░░░░░░░░░░░▌▐░░░░░░░░░░░▌▐░░▌      ▐░▌▐░░░░░░░░░░░▌▐░░░░░░░░░░░▌▐░░░░░░░░░░░▌▐░░▌      ▐░▌\n",
    "▐░█▀▀▀▀▀▀▀█░▌ ▀▀▀▀█░█▀▀▀▀  ▀▀▀▀█░█▀▀▀▀ ▐░█▀▀▀▀▀▀▀▀▀ ▐░▌░▌     ▐░▌ ▀▀▀▀█░█▀▀▀▀  ▀▀▀▀█░█▀▀▀▀ ▐░█▀▀▀▀▀▀▀█░▌▐░▌░▌     ▐░▌\n",
    "▐░▌       ▐░▌     ▐░▌          ▐░▌     ▐░▌          ▐░▌▐░▌    ▐░▌     ▐░▌          ▐░▌     ▐░▌       ▐░▌▐░▌▐░▌    ▐░▌\n",
    "▐░█▄▄▄▄▄▄▄█░▌     ▐░▌          ▐░▌     ▐░█▄▄▄▄▄▄▄▄▄ ▐░▌ ▐░▌   ▐░▌     ▐░▌          ▐░▌     ▐░▌       ▐░▌▐░▌ ▐░▌   ▐░▌\n",
    "▐░░░░░░░░░░░▌     ▐░▌          ▐░▌     ▐░░░░░░░░░░░▌▐░▌  ▐░▌  ▐░▌     ▐░▌          ▐░▌     ▐░▌       ▐░▌▐░▌  ▐░▌  ▐░▌\n",
    "▐░█▀▀▀▀▀▀▀█░▌     ▐░▌          ▐░▌     ▐░█▀▀▀▀▀▀▀▀▀ ▐░▌   ▐░▌ ▐░▌     ▐░▌          ▐░▌     ▐░▌       ▐░▌▐░▌   ▐░▌ ▐░▌\n",
    "▐░▌       ▐░▌     ▐░▌          ▐░▌     ▐░▌          ▐░▌    ▐░▌▐░▌     ▐░▌          ▐░▌     ▐░▌       ▐░▌▐░▌    ▐░▌▐░▌\n",
    "▐░▌       ▐░▌     ▐░▌          ▐░▌     ▐░█▄▄▄▄▄▄▄▄▄ ▐░▌     ▐░▐░▌     ▐░▌      ▄▄▄▄█░█▄▄▄▄ ▐░█▄▄▄▄▄▄▄█░▌▐░▌     ▐░▐░▌\n",
    "▐░▌       ▐░▌     ▐░▌          ▐░▌     ▐░░░░░░░░░░░▌▐░▌      ▐░░▌     ▐░▌     ▐░░░░░░░░░░░▌▐░░░░░░░░░░░▌▐░▌      ▐░░▌\n",
    " ▀         ▀       ▀            ▀       ▀▀▀▀▀▀▀▀▀▀▀  ▀        ▀▀       ▀       ▀▀▀▀▀▀▀▀▀▀▀  ▀▀▀▀▀▀▀▀▀▀▀  ▀        ▀▀\n",
    "                                                                                       \n",
    " ██▓ ███▄    █    ▄▄▄█████▓ ██▀███   ██▓▄▄▄█████▓ ▒█████   ███▄    █ \n",
    "▓██▒ ██ ▀█   █    ▓  ██▒ ▓▒▓██ ▒ ██▒▓██▒▓  ██▒ ▓▒▒██▒  ██▒ ██ ▀█   █ \n",
    "▒██▒▓██  ▀█ ██▒   ▒ ▓██░ ▒░▓██ ░▄█ ▒▒██▒▒ ▓██░ ▒░▒██░  ██▒▓██  ▀█ ██▒\n",
    "░██░▓██▒  ▐▌██▒   ░ ▓██▓ ░ ▒██▀▀█▄  ░██░░ ▓██▓ ░ ▒██   ██░▓██▒  ▐▌██▒\n",
    "░██░▒██░   ▓██░     ▒██▒ ░ ░██▓ ▒██▒░██░  ▒██▒ ░ ░ ████▓▒░▒██░   ▓██░\n",
    "░▓  ░ ▒░   ▒ ▒      ▒ ░░   ░ ▒▓ ░▒▓░░▓    ▒ ░░   ░ ▒░▒░▒░ ░ ▒░   ▒ ▒ \n",
    " ▒ ░░ ░░   ░ ▒░       ░      ░▒ ░ ▒░ ▒ ░    ░      ░ ▒ ▒░ ░ ░░   ░ ▒░\n",
    " ▒ ░   ░   ░ ░      ░        ░░   ░  ▒ ░  ░      ░ ░ ░ ▒     ░   ░ ░ \n",
    " ░           ░                ░      ░               ░ ░           ░                                                                                                                      \n",
    "\n",
    "                                 ,     /~/'   ,--,\n",
    "                               _/`, ,/'/'   /'/~\n",
    "                             .'___|/ /____/'/'   __/|\n",
    "                             /~  __        `\\ /~~, /'\n",
    "                      _,-,__/'  ,       \\   /'/~/ /'\n",
    "                    .~      `   \\_/  / ,     \"~_/'  ,-'~~~~~---,_\n",
    "                    `,               `~    `~~~|   /'    ~~\\__   `~\\_\n",
    "            |~~~/     `~---,__        _,      /'  | /~~\\  _/' ~~\\    `~,\n",
    "            |/\\`\\          /'     _,-~/      /'  .' __ `/'       `~\\    \\\n",
    "   |~~~/       `\\`\\        `-\\/\\/~   /'    .'    |    `| \\/    |    `\\_  |\n",
    "   |/\\`\\         `,`\\              /'      |_  ,' /~\\ /' |' |  `\\     \\~\\|\n",
    "      `\\`\\    _/~~_/~'            /'      /' ~~/     /   `\\ `\\,  | \\   |\n",
    "~/      `\\`\\/~ _/~                ~/~~~~\\/'    `\\__/' \\/\\  `\\_/\\ `\\~~\\ |\n",
    "\\`\\    _/~'    \\               /~~'                `~~~\\`~~~'   `~~'  `'__\n",
    " `\\`\\/~ _/~\\    `\\           /' _/                      `\\        _,-'~~ |\n",
    "   `\\_/~    `\\    `\\       _|--'                          |      `\\     |'\n",
    "              `\\    `\\   /'          _/'                  |       /' /\\|'\n",
    "                /\\/~~\\-/'        _,-'                     |     /' /'  `\n",
    "                |_`\\~~~/`\\     /~                          \\/~~' /'\n",
    "                   |`\\~ \\ `\\   `\\                           `| /'\n",
    "\n",
    "```\n",
    "- by [Marc Lelarge](https://www.di.ens.fr/~lelarge/) - [@marc_lelarge](https://x.com/marc_lelarge)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41532c20-4403-498a-8918-de048cca1f1d",
   "metadata": {},
   "source": [
    "# Standard Softmax\n",
    "\n",
    "The **softmax operator** transforms a vector of real numbers into a probability distribution. For $x_1,\\dots, x_d \\in \\mathbb{R}$, softmax is defined as:\n",
    "$$\n",
    "\\mathrm{softmax}(x_1, \\ldots, x_d)\n",
    "=\n",
    "\\left(\n",
    "\\frac{e^{x_i}}{\\sum_{j=1}^{d} e^{x_j}}\n",
    "\\right)_{i=1}^{d}\n",
    "$$\n",
    "The output satisfies $\\sum_{i=1}^d \\mathrm{softmax}_i(x_1, \\ldots, x_d) = 1$.\n",
    "\n",
    "## Numerical Stability: Safe Softmax\n",
    "\n",
    "Since $x_i$ may be large, computing $e^{x_i}$ directly can cause numerical overflow. To address this, we use the **safe softmax** trick, which exploits the translation invariance of softmax:\n",
    "$$\n",
    "\\frac{e^{x_i}}{\\sum_{j=1}^{d} e^{x_j}}\n",
    "=\n",
    "\\frac{e^{x_i - m}}{\\sum_{j=1}^{d} e^{x_j - m}}\n",
    "$$\n",
    "where\n",
    "$$\n",
    "m = \\max_{j=1,\\ldots,d} x_j\n",
    "$$\n",
    "This ensures $x_i - m \\leq 0$ for all $i$, making the exponential computation numerically stable since $e^{x_i - m} \\in (0, 1]$.\n",
    "\n",
    "## Matrix Extension\n",
    "\n",
    "For matrices $x \\in \\mathbb{R}^{d_1 \\times d_2}$, we extend softmax by applying it **column-wise**. Each $x_i \\in \\mathbb{R}^{d_1}$ denotes the $i$-th column of $x$ (for $i=1,\\ldots,d_2$), and we compute:\n",
    "$$\n",
    "\\mathrm{softmax}(x)_{ij} = \\frac{e^{x_{ij} - m_i}}{\\sum_{k=1}^{d_2} e^{x_{ik} - m_i}}\n",
    "$$\n",
    "where $m_i = \\max_{k=1,\\ldots,d_2} x_{ik}$ is computed along the $i$-th row. For each row index $i$, the outputs form a probability distribution over the columns:\n",
    "$$\n",
    "\\sum_{j=1}^{d_2} \\mathrm{softmax}(x)_{ij} = 1\n",
    "$$\n",
    "To simplify notation, we will use all operations component wise so that we can still write \n",
    "$$\n",
    "\\mathrm{softmax}(x_1, \\ldots, x_{d_2})\n",
    "=\n",
    "\\left(\n",
    "\\frac{e^{x_i- m}}{\\sum_{j=1}^{d_2} e^{x_j- m}}\n",
    "\\right)_{i=1}^{d_2},\n",
    "$$\n",
    "where $e^{x_i}\\in \\mathbb{R}^{d_1}$ is applied component-wise like the maximum $m =\\max_{j=1,\\ldots,d_2} x_j \\in \\mathbb{R}^{d_1}$.\n",
    "\n",
    "The naive implementation of the safe softmax requires 3 passes on the data $x$: the first pass compute the max $m$, the second pass compute the denominator $\\ell = \\sum_{j=1}^{d_2} e^{x_j- m}$ and the last pass compute the softmax. \n",
    "\n",
    "## Algorithm: (safe) softmax\n",
    "\n",
    "**Input:** $x_1, \\ldots, x_{d_2} \\in \\mathbb{R^{d_1}}$  \n",
    "**Output:** $s_1, \\ldots, s_{d_2}$ (softmax values)\n",
    "\n",
    "**Step 1:** Compute the maximum\n",
    "$$\n",
    "\\begin{align}\n",
    "m_0 &= -\\infty \\\\\n",
    "m_i &\\leftarrow \\max(m_{i-1}, x_i) \\quad \\text{for } i = 1, \\ldots, d_2 \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "**Step 2:** Compute the normalizing constant\n",
    "$$\n",
    "\\begin{align}\n",
    "\\ell_0 &= 0 \\\\\n",
    "\\ell_i &\\leftarrow \\ell_{i-1} + e^{x_i - m_{d_2}} \\quad \\text{for } i = 1, \\ldots, d_2\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "**Step 3:** Compute softmax values\n",
    "$$\n",
    "s_i \\leftarrow \\frac{e^{x_i - m_{d_2}}}{\\ell_{d_2}} \\quad \\text{for } i = 1, \\ldots, d_2\n",
    "$$\n",
    "Below is the corresponding code in PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d42e39-80c9-4539-8aa1-d8892b456e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7026e6ee-8d35-4d96-aaef-cde16aa4e47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x, dim=-1):\n",
    "    rescaled_input = x - torch.max(x, dim=dim, keepdim=True)[0]\n",
    "    exponentiated_rescaled_input = torch.exp(rescaled_input)\n",
    "    return exponentiated_rescaled_input / torch.sum(\n",
    "        exponentiated_rescaled_input, dim=dim, keepdim=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb896ddb-17d8-49a1-8c4f-1fdfec1948ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(8,16).cuda()\n",
    "y = softmax(x)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ae9524-1e77-4f94-8aa1-2141dca1941f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ATOL = RTOL = 1e-5\n",
    "torch.testing.assert_close(y.sum(-1), torch.ones(x.shape[0]).cuda(), atol=ATOL, rtol=RTOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38ec22f-4d21-45c8-a7cb-8035456934c5",
   "metadata": {},
   "source": [
    "# Online Softmax\n",
    "\n",
    "Naive implementation requires 3 global memory accesses, we would like to decrease it. In order to compute $\\ell_i = \\sum_{j\\leq i} e^{x_j-m_{d_2}}$, we need to have made one pass in order to compute $m_{d_2}$.\n",
    "Instead, define $\\ell'_i = \\sum_{j\\leq i} e^{x_j-m_i}$ so that we get:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\ell'_i &= \\left( \\sum_{j\\leq i-1} e^{x_j-m_{i-1}} \\right) e^{m_{i-1}-m_i} + e^{x_i-m_i}\\\\\n",
    "&= \\ell'_{i-1}e^{m_{i-1}-m_i} + e^{x_i-m_i},\n",
    "\\end{align}\n",
    "$$\n",
    "and of course $\\ell'_{d_2} = \\ell_{d_2}$ the desired normalizing constant. Since computing $\\ell'_i$ only requires only the quantities $m_{i-1}, m_i$ and $x_i$, it can be computed during the first pass.\n",
    "\n",
    "## Algorithm: online softmax\n",
    "\n",
    "**Input:** $x_1, \\ldots, x_{d_2} \\in \\mathbb{R^{d_1}}$  \n",
    "**Output:** $s_1, \\ldots, s_{d_2}$ (softmax values)\n",
    "\n",
    "**Step 1:** Compute the maximum and the normalizing constant\n",
    "$$\n",
    "\\begin{align}\n",
    "m_0 &= -\\infty \\\\\n",
    "m_i &\\leftarrow \\max(m_{i-1}, x_i) \\quad \\text{for } i = 1, \\ldots, d_2 \\\\\n",
    "\\ell'_0 &= 0 \\\\\n",
    "\\ell'_i &\\leftarrow \\ell'_{i-1}e^{m_{i-1}-m_i} + e^{x_i - m_i} \\quad \\text{for } i = 1, \\ldots, d_2\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "**Step 3:** Compute softmax values\n",
    "$$\n",
    "s_i \\leftarrow \\frac{e^{x_i - m_{d_2}}}{\\ell'_{d_2}} \\quad \\text{for } i = 1, \\ldots, d_2\n",
    "$$\n",
    "Below is the corresponding code in PyTorch (parameter `B` allows to make computations (taking the maximum and summing) in \"blocks\"):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9609f30-0773-45e8-bc51-e9088ee1e3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def online_softmax(x, B=1):\n",
    "    *bs, d = x.shape\n",
    "    device = x.device\n",
    "    Td = math.ceil(d / B)\n",
    "    m_prev = torch.full((*bs, 1), float(\"-inf\"), device=device)  # current max\n",
    "    l_prev = torch.zeros((*bs, 1), device=device)  # current sum of exps\n",
    "\n",
    "    for i in range(Td):\n",
    "        start = i * B\n",
    "        end = min((i + 1) * B, d)\n",
    "        x_block = x[..., start:end]\n",
    "\n",
    "        block_max = x_block.max(dim=-1, keepdim=True).values\n",
    "        m_curr = torch.maximum(m_prev, block_max)\n",
    "\n",
    "        l_prev = l_prev * torch.exp(m_prev - m_curr) + torch.exp(x_block - m_curr).sum(\n",
    "            dim=-1, keepdim=True\n",
    "        )\n",
    "        m_prev = m_curr\n",
    "\n",
    "    softmax_output = torch.empty_like(x)\n",
    "    for i in range(Td):\n",
    "        start = i * B\n",
    "        end = min((i + 1) * B, d)\n",
    "        softmax_output[..., start:end] = torch.exp(x[..., start:end] - m_prev)\n",
    "\n",
    "    return softmax_output / l_prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248df682-95c0-426e-b1de-14fc854b4e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.testing.assert_close(y, online_softmax(x), atol=ATOL, rtol=RTOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16321bf7-2603-4283-b584-fe36e13f9a95",
   "metadata": {},
   "source": [
    "# Triton implementation of online softmax\n",
    "\n",
    "Before using [Triton](https://triton-lang.org/main/index.html), we give a PyTorch implementaion to highlight the main concepts to code in Triton. For more details, we recommend to have a look at the notebook: [Programming on GPUs](https://github.com/dataflowr/notebooks/blob/master/ModuleGPU/GPU_programming_basics.ipynb)\n",
    "\n",
    "## Triton Programming Model\n",
    "\n",
    "Triton uses an **SPMD (Single Program, Multiple Data)** approach where the same kernel code runs in parallel across multiple \"program instances,\" each processing a different block of data.\n",
    "\n",
    "**Key concepts illustrated in the (PyTorch) code below:**\n",
    "\n",
    "1. **Program ID (`fake_pid`)**: Each kernel instance has a unique program ID that determines which data block it processes. In the code, `fake_pid = (i, j)` identifies the batch index `i` and tile index `j`.\n",
    "\n",
    "2. **Block/Tile-based processing**: The input matrix is divided into tiles (e.g., `block_1 × block_2`). Each program instance loads and processes one tile:\n",
    "```python\n",
    "   x_block = x_ptr[fake_pid[0], fake_pid[1] * block_1 : (fake_pid[1] + 1) * block_1, start:end]\n",
    "```\n",
    "\n",
    "3. **Parallel execution**: Multiple kernel instances run concurrently, each with a different `pid`. The shuffling in `online_softmax_fake_triton` simulates this parallelism:\n",
    "```python\n",
    "   all_blocks = [(i, j) for i in range(bs) for j in range(Num_tiles)]\n",
    "   random.shuffle(all_blocks)  # Simulates arbitrary execution order\n",
    "```\n",
    "\n",
    "4. **Memory hierarchy**: Blocks are sized to fit in GPU SRAM (Triton's fast shared memory), though this Python simulation doesn't explicitly model the memory transfer.\n",
    "\n",
    "The programmer writes a single kernel function that operates on one block, and Triton automatically launches many parallel instances across all blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fac2c0-d1ba-43d5-bbcd-e185b8c0a69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def online_softmax_kernel(x_ptr, block_1, block_2, d_2, fake_pid):\n",
    "    device = x_ptr.device\n",
    "    Num_blocks = math.ceil(d_2 / block_2)\n",
    "\n",
    "    m_prev = torch.full((block_1, 1), float(\"-inf\"), device=device)  # current max\n",
    "    l_prev = torch.zeros((block_1, 1), device=device)  # current sum of exps\n",
    "\n",
    "    for i in range(Num_blocks):\n",
    "        start = i * block_2\n",
    "        end = min((i + 1) * block_2, d_2)\n",
    "        x_block = x_ptr[\n",
    "            fake_pid[0], fake_pid[1] * block_1 : (fake_pid[1] + 1) * block_1, start:end\n",
    "        ]\n",
    "\n",
    "        block_max = x_block.max(dim=-1, keepdim=True).values\n",
    "        m_curr = torch.maximum(m_prev, block_max)\n",
    "\n",
    "        l_prev = l_prev * torch.exp(m_prev - m_curr) + torch.exp(x_block - m_curr).sum(\n",
    "            dim=-1, keepdim=True\n",
    "        )\n",
    "        m_prev = m_curr\n",
    "\n",
    "    softmax_output = torch.empty_like(\n",
    "        x_ptr[fake_pid[0], fake_pid[1] * block_1 : (fake_pid[1] + 1) * block_1, :]\n",
    "    )\n",
    "    for i in range(Num_blocks):\n",
    "        start = i * block_2\n",
    "        end = min((i + 1) * block_2, d_2)\n",
    "        x_block = x_ptr[\n",
    "            fake_pid[0], fake_pid[1] * block_1 : (fake_pid[1] + 1) * block_1, start:end\n",
    "        ]\n",
    "        softmax_output[..., start:end] = torch.exp(x_block - m_prev)\n",
    "\n",
    "    return softmax_output / l_prev\n",
    "\n",
    "\n",
    "def online_softmax_fake_triton(x, B=16):\n",
    "    assert x.shape[1] % B == 0, \"d1 must be a multiple of B for fake triton kernel\"\n",
    "    bs, d1, d2 = x.shape\n",
    "    Num_tiles = math.ceil(d1 / B)\n",
    "    softmax_output = torch.empty_like(x)\n",
    "\n",
    "    # Create list of all (batch, tile) pairs and shuffle to show parallelism\n",
    "    all_blocks = [(i, j) for i in range(bs) for j in range(Num_tiles)]\n",
    "    random.shuffle(all_blocks) # Simulates arbitrary execution order\n",
    "\n",
    "    for i, j in all_blocks:\n",
    "        fake_pid = (i, j)\n",
    "        softmax_output[i, j * B : min((j + 1) * B, d1), :] = online_softmax_kernel(\n",
    "            x, B, B, d2, fake_pid\n",
    "        )\n",
    "    return softmax_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8b561c-09c2-4677-a403-e4ddf2163961",
   "metadata": {},
   "outputs": [],
   "source": [
    "B = 8\n",
    "x = torch.randn(2,8,16).cuda()\n",
    "y = softmax(x)\n",
    "torch.testing.assert_close(y, online_softmax_fake_triton(x, B=B), atol=ATOL, rtol=RTOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d1b032-702d-408d-adea-59ccad5d5ca7",
   "metadata": {},
   "source": [
    "## Triton Online Softmax Implementation\n",
    "\n",
    "This kernel computes softmax along dimension `d2` (columns) using the online algorithm, processing the matrix in `BLOCK_1 × BLOCK_2` tiles.\n",
    "\n",
    "### Kernel Launch and Parallelization\n",
    "```python\n",
    "grid = (batch_size, triton.cdiv(d1, BLOCK_1))\n",
    "```\n",
    "\n",
    "The grid defines how many parallel program instances run:\n",
    "- **Dimension 0**: One instance per batch element\n",
    "- **Dimension 1**: One instance per `BLOCK_1` rows\n",
    "\n",
    "Each program instance gets its ID via:\n",
    "```python\n",
    "pid_batch = tl.program_id(0)  # Which batch element\n",
    "pid_row = tl.program_id(1)    # Which block of rows (out of d1/BLOCK_1 blocks)\n",
    "```\n",
    "\n",
    "### Memory Layout: Block Pointers and Strides\n",
    "```python\n",
    "x_block = tl.make_block_ptr(\n",
    "    x_ptr + pid_batch * stride_xbatch,\n",
    "    shape=(d1, d2),\n",
    "    strides=(stride_xrow, stride_xcol),\n",
    "    offsets=(pid_row * BLOCK_1, 0),\n",
    "    block_shape=(BLOCK_1, BLOCK_2),\n",
    "    order=(1, 0),\n",
    ")\n",
    "```\n",
    "\n",
    "**Block pointers** provide a structured way to access multi-dimensional data:\n",
    "- `x_ptr + pid_batch * stride_xbatch`: Start at the correct batch element\n",
    "- `offsets=(pid_row * BLOCK_1, 0)`: Start at row `pid_row * BLOCK_1`, column 0\n",
    "- `block_shape=(BLOCK_1, BLOCK_2)`: Each load reads a `BLOCK_1 × BLOCK_2` tile\n",
    "- `order=(1, 0)`: Memory layout (column-major style indexing)\n",
    "- **Strides** handle arbitrary tensor layouts (contiguous, transposed, etc.)\n",
    "\n",
    "### Algorithm: Two-Pass Online Softmax\n",
    "\n",
    "**Pass 1: Compute global statistics**\n",
    "```python\n",
    "for _ in range(Num_blocks):\n",
    "    x = tl.load(x_block, boundary_check=(0, 1), padding_option=\"zero\")\n",
    "    \n",
    "    block_max = tl.max(x, axis=1)  # Max across columns in this block\n",
    "    m_curr = tl.maximum(m_prev, block_max)  # Update global max\n",
    "    \n",
    "    # Rescale previous sum and add current block's contribution\n",
    "    exp_x_block = tl.exp(x - m_curr[:, None])\n",
    "    l_prev = l_prev * tl.exp(m_prev - m_curr) + tl.sum(exp_x_block, axis=1)\n",
    "    m_prev = m_curr\n",
    "    \n",
    "    x_block = x_block.advance((0, BLOCK_2))  # Move to next column block\n",
    "```\n",
    "\n",
    "This implements equations (7)-(8) from the algorithm:\n",
    "- `m_prev`: Running maximum across all columns seen so far (shape: `BLOCK_1`)\n",
    "- `l_prev`: Running sum of exponentials, rescaled when max updates (shape: `BLOCK_1`)\n",
    "- `x_block.advance((0, BLOCK_2))`: Slides the block pointer right by `BLOCK_2` columns\n",
    "\n",
    "**Pass 2: Compute and store softmax**\n",
    "```python\n",
    "for _ in range(Num_blocks):\n",
    "    x = tl.load(x_block, boundary_check=(0, 1), padding_option=\"zero\")\n",
    "    \n",
    "    tl.store(\n",
    "        softmax_block,\n",
    "        tl.exp(x - m_prev[:, None]) / l_prev[:, None],\n",
    "        boundary_check=(0, 1),\n",
    "    )\n",
    "    \n",
    "    x_block = x_block.advance((0, BLOCK_2))\n",
    "    softmax_block = softmax_block.advance((0, BLOCK_2))\n",
    "```\n",
    "\n",
    "Now that we have `m_prev` and `l_prev`, we compute softmax for each block and write results.\n",
    "\n",
    "### Key Triton Features\n",
    "\n",
    "1. **`tl.constexpr`**: Compile-time constants that enable optimizations\n",
    "```python\n",
    "   d1: tl.constexpr, d2: tl.constexpr, BLOCK_1: tl.constexpr, BLOCK_2: tl.constexpr\n",
    "```\n",
    "\n",
    "2. **`tl.static_assert`**: Compile-time checks ensuring valid block sizes\n",
    "```python\n",
    "   tl.static_assert(d2 % BLOCK_2 == 0, \"d2 must be divisible by BLOCK_2\")\n",
    "```\n",
    "\n",
    "3. **Broadcasting with `[:, None]`**: Expands `m_prev` from shape `(BLOCK_1,)` to `(BLOCK_1, 1)` for broadcasting across columns\n",
    "```python\n",
    "   tl.exp(x - m_prev[:, None])  # Subtract row-wise max from all columns\n",
    "```\n",
    "\n",
    "4. **Boundary checking**: Handles edge cases when dimensions aren't perfect multiples\n",
    "```python\n",
    "   tl.load(x_block, boundary_check=(0, 1), padding_option=\"zero\")\n",
    "```\n",
    "\n",
    "### Why This Design?\n",
    "\n",
    "- **Memory efficiency**: Only loads `BLOCK_1 × BLOCK_2` elements at a time (fits in SRAM)\n",
    "- **Two passes required**: Must see all columns before computing final softmax values\n",
    "- **Parallel across rows**: Different program instances handle different row blocks independently\n",
    "- **Streaming across columns**: Processes columns sequentially within each program instance to stay within memory limits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e9bf61-cbc2-4d8d-84ab-5bd252963adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import triton\n",
    "import triton.language as tl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e1fe7e-6499-4a30-9b02-30bcdd9065f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.jit\n",
    "def online_softmax_triton_kernel(\n",
    "    x_ptr,\n",
    "    softmax_ptr,\n",
    "    stride_xbatch,\n",
    "    stride_xrow,\n",
    "    stride_xcol,\n",
    "    stride_sbatch,\n",
    "    stride_srow,\n",
    "    stride_scol,\n",
    "    d1: tl.constexpr,\n",
    "    d2: tl.constexpr,\n",
    "    BLOCK_1: tl.constexpr,\n",
    "    BLOCK_2: tl.constexpr,\n",
    "):\n",
    "\n",
    "    tl.static_assert(d2 % BLOCK_2 == 0, \"d2 must be divisible by BLOCK_2\")\n",
    "    tl.static_assert(d1 % BLOCK_1 == 0, \"d1 must be divisible by BLOCK_1\")\n",
    "\n",
    "    # Each program handles one block of rows (BLOCK_1 rows)\n",
    "    pid_batch = tl.program_id(0)\n",
    "    pid_row = tl.program_id(1)\n",
    "\n",
    "    x_block = tl.make_block_ptr(\n",
    "        x_ptr + pid_batch * stride_xbatch,\n",
    "        shape=(d1, d2),\n",
    "        strides=(stride_xrow, stride_xcol),\n",
    "        offsets=(pid_row * BLOCK_1, 0),\n",
    "        block_shape=(BLOCK_1, BLOCK_2),\n",
    "        order=(1, 0),\n",
    "    )\n",
    "\n",
    "    # Number of blocks in the column dimension\n",
    "    Num_blocks = tl.cdiv(d2, BLOCK_2)\n",
    "\n",
    "    # Initialize m_prev and l_prev for this block of rows\n",
    "    m_prev = tl.full((BLOCK_1,), float(\"-inf\"), dtype=tl.float32)\n",
    "    l_prev = tl.zeros((BLOCK_1,), dtype=tl.float32)\n",
    "\n",
    "    # First pass: compute global max and sum\n",
    "    for _ in range(Num_blocks):\n",
    "        x = tl.load(x_block, boundary_check=(0, 1), padding_option=\"zero\")\n",
    "\n",
    "        # Compute block max\n",
    "        block_max = tl.max(x, axis=1)\n",
    "        m_curr = tl.maximum(m_prev, block_max)\n",
    "\n",
    "        # Update running sum with rescaling\n",
    "        exp_x_block = tl.exp(x - m_curr[:, None])\n",
    "        l_prev = l_prev * tl.exp(m_prev - m_curr) + tl.sum(exp_x_block, axis=1)\n",
    "        m_prev = m_curr\n",
    "\n",
    "        x_block = x_block.advance((0, BLOCK_2))\n",
    "\n",
    "    # Second pass: compute and store softmax output\n",
    "    softmax_block = tl.make_block_ptr(\n",
    "        softmax_ptr + pid_batch * stride_sbatch,\n",
    "        shape=(d1, d2),\n",
    "        strides=(stride_srow, stride_scol),\n",
    "        offsets=(pid_row * BLOCK_1, 0),\n",
    "        block_shape=(BLOCK_1, BLOCK_2),\n",
    "        order=(1, 0),\n",
    "    )\n",
    "    x_block = tl.make_block_ptr(\n",
    "        x_ptr + pid_batch * stride_xbatch,\n",
    "        shape=(d1, d2),\n",
    "        strides=(stride_xrow, stride_xcol),\n",
    "        offsets=(pid_row * BLOCK_1, 0),\n",
    "        block_shape=(BLOCK_1, BLOCK_2),\n",
    "        order=(1, 0),\n",
    "    )\n",
    "    # tl.device_print(\"m_prev:\", m_prev)\n",
    "    # tl.device_print(\"l_prev:\", l_prev)\n",
    "\n",
    "    for _ in range(Num_blocks):\n",
    "        x = tl.load(x_block, boundary_check=(0, 1), padding_option=\"zero\")\n",
    "        # Compute softmax for this block\n",
    "        tl.store(\n",
    "            softmax_block,\n",
    "            tl.exp(x - m_prev[:, None]) / l_prev[:, None],\n",
    "            boundary_check=(0, 1),\n",
    "        )\n",
    "\n",
    "        x_block = x_block.advance((0, BLOCK_2))\n",
    "        softmax_block = softmax_block.advance((0, BLOCK_2))\n",
    "\n",
    "\n",
    "def online_softmax_triton(x, BLOCK_1=16, BLOCK_2=16):\n",
    "    \"\"\"\n",
    "    Compute softmax using Triton kernel with online algorithm.\n",
    "\n",
    "    Args:\n",
    "        x: Input tensor of shape (batch_size, d1, d2)\n",
    "        BLOCK_1: Block size for dimension d1 (rows)\n",
    "        BLOCK_2: Block size for dimension d2 (columns, softmax dimension)\n",
    "    \"\"\"\n",
    "    batch_size, d1, d2 = x.shape\n",
    "    softmax_output = torch.empty_like(x)\n",
    "\n",
    "    # Calculate grid dimensions\n",
    "    grid = (batch_size, triton.cdiv(d1, BLOCK_1))\n",
    "\n",
    "    # Launch kernel\n",
    "    online_softmax_triton_kernel[grid](\n",
    "        x,\n",
    "        softmax_output,\n",
    "        x.stride(0),\n",
    "        x.stride(1),\n",
    "        x.stride(2),\n",
    "        softmax_output.stride(0),\n",
    "        softmax_output.stride(1),\n",
    "        softmax_output.stride(2),\n",
    "        d1,\n",
    "        d2,\n",
    "        BLOCK_1=BLOCK_1,\n",
    "        BLOCK_2=BLOCK_2,\n",
    "    )\n",
    "\n",
    "    return softmax_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6f7e1d-27cd-44e0-93f3-b287c66dea36",
   "metadata": {},
   "outputs": [],
   "source": [
    "B = 8\n",
    "x = torch.randn(2,16,32).cuda()\n",
    "y = softmax(x)\n",
    "torch.testing.assert_close(y, online_softmax_triton(x, BLOCK_1=B, BLOCK_2=2*B), atol=ATOL, rtol=RTOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332b8b7b-b3f6-4416-ad0f-c2ccac7a4c94",
   "metadata": {},
   "source": [
    "## GPU Benchmarking with CUDA Events\n",
    "\n",
    "### Why `torch.cuda.synchronize()` is Critical\n",
    "\n",
    "**GPU operations are asynchronous:**\n",
    "- When you call a CUDA operation (e.g., `x @ y`), Python immediately returns without waiting\n",
    "- The GPU kernel is queued and executes later\n",
    "- The CPU continues running Python code while GPU works in the background\n",
    "\n",
    "**Example without synchronization (WRONG):**\n",
    "```python\n",
    "start = time.time()\n",
    "result = heavy_gpu_computation()  # Returns immediately!\n",
    "end = time.time()\n",
    "print(end - start)  # ❌ Measures ~0ms (only kernel launch overhead)\n",
    "```\n",
    "\n",
    "### The Role of Each `synchronize()` Call\n",
    "\n",
    "**First `torch.cuda.synchronize()` (before `start.record()`):**\n",
    "```python\n",
    "torch.cuda.synchronize()  # Ensure clean slate\n",
    "start.record()\n",
    "```\n",
    "- **Purpose:** Wait for all previous GPU operations to complete\n",
    "- **Why needed:** Without this, previous iterations' work might still be running\n",
    "- **Effect:** Ensures we're measuring only `fn()`, not leftover work from earlier\n",
    "\n",
    "**Second `torch.cuda.synchronize()` (after `end.record()`):**\n",
    "```python\n",
    "end.record()\n",
    "torch.cuda.synchronize()  # Wait for fn() to finish\n",
    "times.append(start.elapsed_time(end))\n",
    "```\n",
    "- **Purpose:** Wait for `fn()` to actually complete\n",
    "- **Why needed:** Without this, `elapsed_time()` might be called before the work is done\n",
    "- **Effect:** Ensures timing measurement is accurate\n",
    "\n",
    "### CUDA Events for Accurate Timing\n",
    "```python\n",
    "start = torch.cuda.Event(enable_timing=True)\n",
    "end = torch.cuda.Event(enable_timing=True)\n",
    "```\n",
    "\n",
    "**CUDA Events** are markers placed in the GPU command stream:\n",
    "- `start.record()`: Places a timestamp marker *on the GPU*\n",
    "- `end.record()`: Places another timestamp marker *on the GPU*\n",
    "- `start.elapsed_time(end)`: Computes GPU-side time between markers\n",
    "\n",
    "**Why use Events instead of `time.time()`?**\n",
    "- Events measure actual GPU execution time (excluding CPU-GPU communication)\n",
    "- `time.time()` would include Python overhead, kernel launch latency, etc.\n",
    "- Events are synchronized with the GPU command stream\n",
    "\n",
    "### What Happens Without Synchronization?\n",
    "```python\n",
    "# BAD: No synchronization\n",
    "start.record()\n",
    "fn()\n",
    "end.record()\n",
    "# start.elapsed_time(end) called immediately\n",
    "# ❌ fn() might not have started yet!\n",
    "```\n",
    "\n",
    "**Timeline without sync:**\n",
    "```\n",
    "CPU:  record_start -> fn() -> record_end -> elapsed_time()\n",
    "GPU:                    ... still working on previous iteration ...\n",
    "```\n",
    "Result: Timing is meaningless\n",
    "\n",
    "**Timeline with sync:**\n",
    "```\n",
    "CPU:  sync -> record_start -> fn() -> record_end -> sync -> elapsed_time()\n",
    "GPU:  [wait] -> execute fn() -> [done]\n",
    "```\n",
    "Result: Accurate measurement\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "1. **Always synchronize** before and after the code you're timing\n",
    "2. **Use CUDA Events** for GPU timing (not `time.time()`)\n",
    "3. **Warmup iterations**: Run `fn()` a few times before timing to avoid one-time costs (JIT compilation, memory allocation)\n",
    "4. **Multiple iterations**: Run many times and compute statistics (median, percentiles) to account for variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae800459-f1e4-4ea6-8779-a6fee7560524",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_loop(fn, iters, warmup=5):\n",
    "    # Warmup phase\n",
    "    for _ in range(warmup):\n",
    "        fn()\n",
    "    torch.cuda.synchronize()\n",
    "    \n",
    "    # Timing phase\n",
    "    times = []\n",
    "    for _ in range(iters):\n",
    "        start = torch.cuda.Event(enable_timing=True)\n",
    "        end = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "        torch.cuda.synchronize()  # Wait for previous work to finish\n",
    "        start.record()            # Record start event\n",
    "        fn()                      # Execute the function\n",
    "        end.record()              # Record end event\n",
    "        torch.cuda.synchronize()  # Wait for fn() to finish\n",
    "        \n",
    "        times.append(start.elapsed_time(end))\n",
    "    return times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf397de-fd4d-474d-ba83-3a8a9f21afd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "dtype = torch.float32\n",
    "batch_size = 8\n",
    "d1 = 2048\n",
    "d2 = 8192\n",
    "B = 32\n",
    "x = torch.randn(batch_size, d1, d2, device=device, dtype=dtype)\n",
    "\n",
    "times_triton = time_loop(lambda: online_softmax_triton(x, BLOCK_1=B, BLOCK_2=B), 10)\n",
    "\n",
    "times_naive = time_loop(lambda: softmax(x), 10)\n",
    "\n",
    "times_pytorch = time_loop(lambda: F.softmax(x, dim=-1), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4074cae2-fa4f-435f-ac7f-d90595408ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"NAIVE bench ms_mean: {np.mean(times_naive)} and ms_std: {np.std(times_naive)}\")\n",
    "print(f\"TRITON bench ms_mean: {np.mean(times_triton)} and ms_std: {np.std(times_triton)}\")\n",
    "print(f\"PYTORCH bench ms_mean: {np.mean(times_pytorch)} and ms_std: {np.std(times_pytorch)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1377566-6041-46b5-96b3-50fd53daca3c",
   "metadata": {},
   "source": [
    "## Operator fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fd3e55-ce32-40b6-9415-824636c55754",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_mult(x, V, dim=-1):\n",
    "    #softmax_output = softmax(x, dim=dim)\n",
    "    #return softmax_output @ V\n",
    "    return F.softmax(x, dim=dim) @ V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3126a204-d5d4-486f-a1a9-1ddc8f0c463c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.jit\n",
    "def fused_softmax_triton_kernel(\n",
    "    x_ptr,\n",
    "    V_ptr,\n",
    "    output_ptr,\n",
    "    stride_xbatch,\n",
    "    stride_xrow,\n",
    "    stride_xcol,\n",
    "    stride_Vbatch,\n",
    "    stride_Vrow,\n",
    "    stride_Vcol,\n",
    "    stride_outbatch,\n",
    "    stride_outrow,\n",
    "    stride_outcol,\n",
    "    d1: tl.constexpr,\n",
    "    d2: tl.constexpr,\n",
    "    d3: tl.constexpr,\n",
    "    BLOCK_1: tl.constexpr,\n",
    "    BLOCK_2: tl.constexpr,\n",
    "):\n",
    "    tl.static_assert(d2 % BLOCK_2 == 0, \"d2 must be divisible by BLOCK_2\")\n",
    "    tl.static_assert(d1 % BLOCK_1 == 0, \"d1 must be divisible by BLOCK_1\")\n",
    "\n",
    "    pid_batch = tl.program_id(0)\n",
    "    pid_row = tl.program_id(1)\n",
    "\n",
    "    x_block = tl.make_block_ptr(\n",
    "        x_ptr + pid_batch * stride_xbatch,\n",
    "        shape=(d1, d2),\n",
    "        strides=(stride_xrow, stride_xcol),\n",
    "        offsets=(pid_row * BLOCK_1, 0),\n",
    "        block_shape=(BLOCK_1, BLOCK_2),\n",
    "        order=(1, 0),\n",
    "    )\n",
    "    \n",
    "    V_block = tl.make_block_ptr(\n",
    "        V_ptr + pid_batch * stride_Vbatch,\n",
    "        shape=(d2, d3),\n",
    "        strides=(stride_Vrow, stride_Vcol),\n",
    "        offsets=(0, 0),\n",
    "        block_shape=(BLOCK_2, d3),\n",
    "        order=(1, 0),\n",
    "    )\n",
    "\n",
    "    output_block = tl.make_block_ptr(\n",
    "        output_ptr + pid_batch * stride_outbatch,\n",
    "        shape=(d1, d3),\n",
    "        strides=(stride_outrow, stride_outcol),\n",
    "        offsets=(pid_row * BLOCK_1, 0),\n",
    "        block_shape=(BLOCK_1, d3),\n",
    "        order=(1, 0),\n",
    "    )\n",
    "\n",
    "    Num_blocks = tl.cdiv(d2, BLOCK_2)\n",
    "\n",
    "    # fp32 accumulators for numerical stability\n",
    "    m_prev = tl.full((BLOCK_1,), float(\"-inf\"), dtype=tl.float32)\n",
    "    l_prev = tl.zeros((BLOCK_1,), dtype=tl.float32)\n",
    "    out_prev = tl.zeros((BLOCK_1, d3), dtype=tl.float32)\n",
    "\n",
    "    for _ in range(Num_blocks):\n",
    "        x = tl.load(x_block, boundary_check=(0, 1), padding_option=\"zero\")\n",
    "        v = tl.load(V_block, boundary_check=(0, 1), padding_option=\"zero\")\n",
    "\n",
    "        # Compute block max\n",
    "        block_max = tl.max(x, axis=1)\n",
    "        m_curr = tl.maximum(m_prev, block_max)\n",
    "\n",
    "        # Update running sum with rescaling\n",
    "        exp_x_block = tl.exp(x - m_curr[:, None])\n",
    "        l_curr = l_prev * tl.exp(m_prev - m_curr) + tl.sum(exp_x_block, axis=1)\n",
    "        \n",
    "        # Scale and accumulate \n",
    "        # depending on hardware:\n",
    "        # on Turing (T4, RTX8000), cast for dot product only because of this bug: https://github.com/triton-lang/triton/issues/5557\n",
    "        # on Hopper (H100), remove casting\n",
    "        scale = l_prev / l_curr * tl.exp(m_prev - m_curr)\n",
    "        normalized = (exp_x_block / l_curr[:, None]).to(tl.float16)\n",
    "        out_prev = out_prev * scale[:, None] + tl.dot(normalized, v.to(tl.float16)).to(tl.float32)\n",
    "\n",
    "        m_prev = m_curr\n",
    "        l_prev = l_curr\n",
    "\n",
    "        x_block = x_block.advance((0, BLOCK_2))\n",
    "        V_block = V_block.advance((BLOCK_2, 0))\n",
    "\n",
    "    tl.store(output_block, out_prev, boundary_check=(0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9244928a-5fb9-441b-b63d-311ff0f772cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fused_softmax_triton(x, V, BLOCK_1=16, BLOCK_2=16):\n",
    "    \n",
    "    batch_size, d1, d2 = x.shape\n",
    "    bs, d2 ,d3 = V.shape\n",
    "    assert batch_size == bs, \"Batch size of x and V must match\"\n",
    "    assert d2 == d2, \"d2 of x and V must match\"\n",
    "    fused_softmax_output = torch.empty((batch_size, d1, d3), device=x.device, dtype=x.dtype)\n",
    "\n",
    "    # Calculate grid dimensions\n",
    "    grid = (batch_size, triton.cdiv(d1, BLOCK_1))\n",
    "\n",
    "    # Launch kernel\n",
    "    fused_softmax_triton_kernel[grid](\n",
    "        x,\n",
    "        V,\n",
    "        fused_softmax_output,\n",
    "        x.stride(0),\n",
    "        x.stride(1),\n",
    "        x.stride(2),\n",
    "        V.stride(0),\n",
    "        V.stride(1),\n",
    "        V.stride(2),\n",
    "        fused_softmax_output.stride(0),\n",
    "        fused_softmax_output.stride(1),\n",
    "        fused_softmax_output.stride(2),\n",
    "        d1,\n",
    "        d2,\n",
    "        d3,\n",
    "        BLOCK_1=BLOCK_1,\n",
    "        BLOCK_2=BLOCK_2,\n",
    "    )\n",
    "\n",
    "    return fused_softmax_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca713cb-79d5-4759-859c-da6d627d34ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "dtype = torch.float32\n",
    "batch_size = 8\n",
    "d1 = 2048\n",
    "d2 = 8192\n",
    "d3 = 64\n",
    "B = 16\n",
    "x = torch.randn(batch_size, d1, d2, device=device, dtype=dtype)\n",
    "V = torch.randn(batch_size, d2, d3, device=device, dtype=dtype)\n",
    "times_triton = time_loop(lambda: fused_softmax_triton(x, V, BLOCK_1=B, BLOCK_2=B), 10)\n",
    "times_pytorch = time_loop(lambda: softmax_mult(x,V), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e796cd4e-d140-4119-bd64-1d98a0b4e4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"TRITON bench ms_mean: {np.mean(times_triton)} and ms_std: {np.std(times_triton)}\")\n",
    "print(f\"PYTORCH bench ms_mean: {np.mean(times_pytorch)} and ms_std: {np.std(times_pytorch)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6448620e-6a06-4b19-954c-22465fae6e69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chess",
   "language": "python",
   "name": "chess"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
